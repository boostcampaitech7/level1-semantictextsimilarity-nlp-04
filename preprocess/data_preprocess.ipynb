{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리/증강 데이터셋 버전 \n",
    "#### 형식\n",
    "- v{대분류}_{인덱스}\n",
    "#### 대분류\n",
    "- v1: 전처리만 적용\n",
    "- v2: v1 + swap만 적용 \n",
    "- v3: v2 + 데이터 증강 기법 적용\n",
    "- v4: v3 + 매우 실험적인 전처리/증강 기법 적용\n",
    "#### 인덱스\n",
    "- 01부터 99까지 차례로 순서를 매김\n",
    "#### 예시\n",
    "- 맞춤법 교정 데이터셋 -> v1_01\n",
    "- 특수문자 제거 데이터셋 -> v1_02\n",
    "- 특수문자 제거, 띄어쓰기 교정, swap 데이터셋 -> v2_01\n",
    "- 맞춤법 교정, swap, RS, SR -> v3_01\n",
    "- ChatGPT api 증강 데이터셋 -> v4_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# 모든 경고 메시지 무시\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_count(df):\n",
    "    \"\"\"라벨 분포 확인용\"\"\"\n",
    "    print(len(df))\n",
    "    df[\"label_int\"] = pd.cut(\n",
    "        df[\"label\"],\n",
    "        bins=[x for x in range(6)],\n",
    "        labels=[x for x in range(5)],\n",
    "        right=False,\n",
    "    )\n",
    "    print(df.groupby(\"label_int\")[\"id\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_special_letters(df):\n",
    "    \"\"\"특수문자 제거 \\n \n",
    "    초성(ㄱ-ㅎ), 중성(ㅏ-ㅣ), 완성된 한글(가-힣), 알파벳(A-Za-z), 숫자(0-9), 그리고 공백(\\s)만 허용\"\"\"\n",
    "    df[\"sentence_1\"] = df[\"sentence_1\"].str.replace(\n",
    "        r\"[^A-Za-z0-9가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", regex=True\n",
    "    )\n",
    "    df[\"sentence_2\"] = df[\"sentence_2\"].str.replace(\n",
    "        r\"[^A-Za-z0-9가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", regex=True\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanspell import passportKey, spell_checker\n",
    "\n",
    "# passportKey 설정\n",
    "passportKey.init()\n",
    "\n",
    "\n",
    "def spell_check(df):\n",
    "    \"\"\"맞춤법(오탈자, 띄어쓰기 등 전부) 교정\"\"\"\n",
    "    df[\"sentence_1\"] = df.apply(lambda row: spell_checker.check(row[\"sentence_1\"]).checked, axis=1)\n",
    "    df[\"sentence_2\"] = df.apply(lambda row: spell_checker.check(row[\"sentence_2\"]).checked, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykospacing\n",
    "\n",
    "\n",
    "def spacing_text(df):\n",
    "    \"\"\"띄어쓰기만 교정\"\"\"\n",
    "    spacing = pykospacing.Spacing()\n",
    "    df[\"sentence_1\"] = df[\"sentence_1\"].map(spacing)\n",
    "    df[\"sentence_2\"] = df[\"sentence_2\"].map(spacing)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap(df):\n",
    "    \"\"\"sentence 1과 2를 교환한 데이터 추가\"\"\"\n",
    "    df_swaped = df.rename(\n",
    "        columns={\"sentence_1\": \"sentence_2\", \"sentence_2\": \"sentence_1\"}\n",
    "    )\n",
    "    return pd.concat([df, df_swaped])\n",
    "\n",
    "\n",
    "def swap_over_one_label(df):\n",
    "    \"\"\"sentence 1과 2를 교환한 데이터 추가\"\"\"\n",
    "    df_swaped = df.rename(\n",
    "        columns={\"sentence_1\": \"sentence_2\", \"sentence_2\": \"sentence_1\"}\n",
    "    )\n",
    "    df_filtered = df_swaped[df_swaped[\"label\"] >= 1]\n",
    "\n",
    "    return pd.concat([df, df_filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eda import eda\n",
    "\n",
    "# 커스텀된 EDA 함수 말고 원본 패키지 사용하고 싶다면 https://github.com/toriving/KoEDA\n",
    "def apply_eda(df, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, num_aug=2):\n",
    "    \"\"\"\n",
    "    EDA 적용 함수 \\n \n",
    "    주의: 한글 제외한 나머지 문자 제거됨 \\n\n",
    "    alpha_sr: 특정 단어를 유의어로 교체할 확률 \\n\n",
    "    alpha_ri: 임의의 단어를 삽입할 확률 \\n\n",
    "    alpha_rs: 문장 내 임의의 두 단어의 위치를 바꿀 확률 \\n\n",
    "    num_aug: 데이터 증강하는 개수 \\n\n",
    "    \"\"\"\n",
    "    def _conditional_eda(row, column_name):\n",
    "        if row[\"label\"] >= 0:  \n",
    "            return eda.EDA(\n",
    "                row[column_name], alpha_sr, alpha_ri, alpha_rs, num_aug)\n",
    "        else:\n",
    "            return [row[column_name]]\n",
    "\n",
    "    def _replace_person_token(df):\n",
    "        \"\"\"Speicial 토큰 처리: <PERSON> -> 궯궯궯\"\"\"\n",
    "        df[\"sentence_1\"] = df[\"sentence_1\"].str.replace(\"<PERSON>\", \"궯궯궯\")\n",
    "        df[\"sentence_2\"] = df[\"sentence_2\"].str.replace(\"<PERSON>\", \"궯궯궯\")\n",
    "        return df\n",
    "\n",
    "    def _recover_person_token(df):\n",
    "        \"\"\"Speicial 토큰 처리: 궯궯궯 -> <PERSON>\"\"\"\n",
    "        df[\"sentence_1\"] = df[\"sentence_1\"].str.replace(\"궯궯궯\", \"<PERSON>\")\n",
    "        df[\"sentence_2\"] = df[\"sentence_2\"].str.replace(\"궯궯궯\", \"<PERSON>\")\n",
    "        return df\n",
    "\n",
    "    df = _replace_person_token(df)\n",
    "    df[\"sentence_1\"] = df.apply(lambda row: _conditional_eda(row, \"sentence_1\"), axis=1)\n",
    "    df = df.explode(\"sentence_1\").reset_index(drop=True)\n",
    "    df[\"sentence_2\"] = df.apply(lambda row: _conditional_eda(row, \"sentence_2\"), axis=1)\n",
    "    df = df.explode(\"sentence_2\").reset_index(drop=True)\n",
    "    df = _recover_person_token(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 및 증강 적용한 csv 파일 생성 (원하는 함수를 선택하여 사용)\n",
    "def make(df, df_name):\n",
    "    df = replace_special_letters(df)\n",
    "    df = spell_check(df)\n",
    "    df = spacing_text(df)\n",
    "    df = swap(df)\n",
    "    df = apply_eda(df)\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(f\"./{df_name}.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_v1(df, df_name):\n",
    "    df = replace_special_letters(df)\n",
    "    df = spell_check(df)\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(f\"../data/{df_name}.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_v1_02(df, df_name):\n",
    "    df = spell_check(df)\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(f\"../data/{df_name}.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_v2(df, df_name):\n",
    "    df = replace_special_letters(df)\n",
    "    df = spell_check(df)\n",
    "    df = swap(df)\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(f\"../data/{df_name}.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_v2_02(df, df_name):\n",
    "    df = replace_special_letters(df)\n",
    "    df = spell_check(df)\n",
    "    df = swap_over_one_label(df)\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(f\"../data/{df_name}.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_v2_03(df, df_name):\n",
    "    df = spell_check(df)\n",
    "    df = swap_over_one_label(df)\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(f\"../data/{df_name}.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_v3(df, df_name):\n",
    "    df = replace_special_letters(df)\n",
    "    df = spell_check(df)\n",
    "    df = swap_over_one_label(df)\n",
    "    df = apply_eda(df, alpha_sr=0.3, alpha_ri=0.3, alpha_rs=0.3)\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(f\"../data/{df_name}.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "not well-formed (invalid token): line 1, column 14 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda/envs/ame/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[10], line 4\u001b[0m\n    train_v1 = make_v1_02(train, \"train_v1_02\")\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[9], line 10\u001b[0m in \u001b[1;35mmake_v1_02\u001b[0m\n    df = spell_check(df)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[4], line 9\u001b[0m in \u001b[1;35mspell_check\u001b[0m\n    df[\"sentence_1\"] = df.apply(lambda row: spell_checker.check(row[\"sentence_1\"]).checked, axis=1)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda/envs/ame/lib/python3.11/site-packages/pandas/core/frame.py:10374\u001b[0m in \u001b[1;35mapply\u001b[0m\n    return op.apply().__finalize__(self, method=\"apply\")\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda/envs/ame/lib/python3.11/site-packages/pandas/core/apply.py:916\u001b[0m in \u001b[1;35mapply\u001b[0m\n    return self.apply_standard()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda/envs/ame/lib/python3.11/site-packages/pandas/core/apply.py:1063\u001b[0m in \u001b[1;35mapply_standard\u001b[0m\n    results, res_index = self.apply_series_generator()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda/envs/ame/lib/python3.11/site-packages/pandas/core/apply.py:1081\u001b[0m in \u001b[1;35mapply_series_generator\u001b[0m\n    results[i] = self.func(v, *self.args, **self.kwargs)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[4], line 9\u001b[0m in \u001b[1;35m<lambda>\u001b[0m\n    df[\"sentence_1\"] = df.apply(lambda row: spell_checker.check(row[\"sentence_1\"]).checked, axis=1)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/DEV/git/level1-semantictextsimilarity-nlp-04/preprocess/hanspell/spell_checker.py:67\u001b[0m in \u001b[1;35mcheck\u001b[0m\n    \"checked\": _remove_tags(html),\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/DEV/git/level1-semantictextsimilarity-nlp-04/preprocess/hanspell/spell_checker.py:27\u001b[0m in \u001b[1;35m_remove_tags\u001b[0m\n    result = \"\".join(ET.fromstring(text).itertext())\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/miniconda/envs/ame/lib/python3.11/xml/etree/ElementTree.py:1350\u001b[0;36m in \u001b[0;35mXML\u001b[0;36m\n\u001b[0;31m    parser.feed(text)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m not well-formed (invalid token): line 1, column 14\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/raw/train.csv\", encoding=\"UTF-8\")\n",
    "dev = pd.read_csv(\"../data/raw/dev.csv\", encoding=\"UTF-8\")\n",
    "\n",
    "train_v1 = make_v1_02(train, \"train_v1_02\")\n",
    "dev_v1 = make_v1_02(dev, \"dev_v1_02\")\n",
    "\n",
    "print_label_count(train_v1)\n",
    "print_label_count(dev_v1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
